# R√©colte et pr√©diction des embouteillages

![technos utilis√©es](./docs/data.png)

# Pipeline de Donn√©es Bison Fut√© avec Dagster

Ce projet impl√©mente une pipeline ETL compl√®te permettant d'extraire, transformer et analyser et pr√©dire les donn√©es d'√©v√©nements routiers depuis l'API Bison Fut√©.

## Fonctionnalit√©s

- Extraction automatique des fichiers XML d'√©v√©nements routiers depuis l'API Bison Fut√©
- Parsing et transformation des donn√©es XML en format structur√©
- Nettoyage et normalisation des donn√©es
- Entra√Ænement et utilisation de mod√®les pr√©dictifs pour anticiper les bouchons
- Analyses statistiques des √©v√©nements routiers
- Planification automatique des actualisations de donn√©es

## Architecture

Le projet suit une architecture moderne avec trois composantes principales:

#### Acquisition des donn√©es

- R√©cup√©ration des fichiers XML
- Parsing XML et extraction des donn√©es structur√©es

#### Pr√©paration et analyse

- Nettoyage des donn√©es XML
- Pr√©paration des donn√©es pour mod√©lisation

#### Mod√©lisation et pr√©diction

- Entra√Ænement de mod√®les pr√©dictifs
- G√©n√©ration de pr√©dictions de trafic pour les routes principales

---

## Technologies utilis√©es

| Cat√©gorie                  | Technologie  | Description                                |
| -------------------------- | ------------ | ------------------------------------------ |
| **Orchestration**          | Dagster      | Orchestration de la pipeline de donn√©es    |
| **Base de donn√©es**        | MongoDB      | Stockage persistant des donn√©es            |
| **Traitement des donn√©es** | pandas       | Manipulation et transformation des donn√©es |
| **Parsing XML**            | lxml         | Traitement des fichiers XML                |
| **Machine Learning**       | scikit-learn | Mod√®les de pr√©diction du trafic            |
| **Conteneurisation**       | Docker       | Conteneurisation de l'application          |

---

## üöÄ Installation

### Pr√©requis

- Docker et Docker Compose
- Git

### Faire tourner le projet

1. Cloner le d√©p√¥t

```sh
git clone git@github.com:SUPMAP-DELTA-FORCE/supmap_data.git
cd dagster
```

2. Remplir les variables d'environnement

```sh
cp ./dagster/.env.example ./dagster/.env    # dont forget to fill in the values of each secret
```

3. Lancer les conteneurs Docker

```sh
docker compose up -d
task reset-no-cache
```

## Utilisation

Vous pouvez d√©sormais acc√©der √† l'interface Dagster

http://localhost:3000

Mat√©rialisation manuelle des assets

- Acc√©der √† l'interface Dagster
- Aller dans la section "Assets"
- Cliquer sur "Materialize all"

## Structure des assets

Le pipeline est compos√© de plusieurs assets qui s'ex√©cutent dans un ordre sp√©cifique pour traiter les donn√©es de trafic routier. Voici la structure d√©taill√©e :

```mermaid
graph TB
    subgraph Acquisition
        A1[recuperer_xml_bison_fute] -->|Fichiers XML| A2[parser_xml_bison_fute]
    end

    subgraph Preparation
        A2 -->|Donn√©es structur√©es| B1[nettoyer_donnees_xml]
        B1 -->|Donn√©es nettoy√©es| B2[preparer_donnees]
    end

    subgraph Modelisation
        B2 -->|Features| C1[entrainer_modele]
    end

    subgraph Prediction
        C1 -->|Mod√®le entra√Æn√©| D1[predire_bouchons_journee]
    end
```

### Description des assets

#### 1. Acquisition des donn√©es

- **recuperer_xml_bison_fute** : R√©cup√®re les fichiers XML depuis l'API Bison Fut√©
- **parser_xml_bison_fute** : Transforme les fichiers XML en donn√©es structur√©es

#### 2. Pr√©paration des donn√©es

- **nettoyer_donnees_xml** : Nettoie et standardise les donn√©es extraites
- **preparer_donnees** : Pr√©pare les donn√©es pour l'entra√Ænement du mod√®le

#### 3. Mod√©lisation

- **entrainer_modele** : Entra√Æne le mod√®le pr√©dictif sur les donn√©es pr√©par√©es

#### 4. Pr√©diction

- **predire_bouchons_journee** : G√©n√®re des pr√©dictions de trafic pour les routes nationales et autoroutes

### Utilisation

Pour ex√©cuter manuellement le pipeline :

1. Acc√©der √† l'interface Dagster (http://localhost:3000)
2. Aller dans la section "Assets"
3. Cliquer sur "Materialize all"

---

## Architecture technique

#### Extraction des donn√©es

- Source : API Bison Fut√© (tipi.bison-fute.gouv.fr)
- Donn√©es : Fichiers XML DATEX II contenant des informations sur le trafic routier

#### Stockage

- MongoDB pour le stockage des donn√©es transform√©es

#### Transformation

- Parsing XML avec lxml
- Nettoyage et standardisation des donn√©es avec pandas
- Pr√©paration des features pour l'apprentissage machine

#### Mod√©lisation

- Mod√®les pr√©dictifs bas√©s sur scikit-learn
- Entra√Ænement sur les donn√©es historiques
- Pr√©diction des conditions de trafic futures

#### Monitoring

- Logs d√©taill√©s accessibles via l'interface Dagster

```mermaid
graph TB
    subgraph Sources
        A[API Bison Fut√©] -->|XML DATEX II| B[Fichiers XML]
    end

    subgraph Traitement
        B --> C[Parsing XML<br/>lxml]
        C --> D[Nettoyage<br/>pandas]
        D --> E[Pr√©paration<br/>features]
    end

    subgraph Stockage
        E --> F[MongoDB]
    end

    subgraph Machine Learning
        F --> G[Entra√Ænement<br/>scikit-learn]
        G --> H[Pr√©dictions]
    end

    subgraph Monitoring
        I[Dagster UI] -->|Logs| J[Monitoring]
    end
```

---

## Scheduling

### La schedule (bison_fute_daily_schedule)

La schedule est configur√©e pour ex√©cuter automatiquement le job mon_job tous les jours √† 6h00 du matin.
En pratique, cela signifie que:
Chaque jour √† 6h00 du matin, Dagster d√©marrera automatiquement le pipeline de traitement

Ce pipeline ex√©cutera s√©quentiellement toutes les √©tapes d√©finies dans le job:

- R√©cup√©ration des fichiers XML
- Analyse (parsing) de ces fichiers
- Nettoyage des donn√©es extraites
- Export des donn√©es en CSV
- Chargement des donn√©es dans MongoDB
  Cette automatisation garantit que le traitement s'ex√©cute quotidiennement sans intervention manuelle.

```mermaid
graph LR
    A[6h00] --> B[Lancement du Pipeline]
    B --> C[R√©cup√©ration XML]
    C --> D[Analyse/Parsing]
    D --> E[Nettoyage]
    E --> F[Export CSV]
    F --> G[Chargement MongoDB]
```

### Le sensor (bison_fute_file_sensor)

Le sensor surveille en continu l'arriv√©e de nouveaux fichiers XML dans un r√©pertoire sp√©cifi√©. Son fonctionnement est le suivant:

Le sensor s'active p√©riodiquement (typiquement toutes les 30 secondes par d√©faut)
√Ä chaque activation, il v√©rifie si de nouveaux fichiers XML sont apparus dans le r√©pertoire surveill√©
S'il d√©tecte de nouveaux fichiers, il:

- Enregistre ces fichiers comme "d√©j√† trait√©s" pour ne pas les retraiter √† l'avenir
- D√©clenche imm√©diatement une ex√©cution du job pour traiter ces nouveaux fichiers
- Si aucun nouveau fichier n'est d√©tect√©, il ne fait rien et attendra la prochaine v√©rification

```mermaid
graph TD
    A[Sensor Actif] -->|Toutes les 30s| B{V√©rification<br/>Nouveaux fichiers?}
    B -->|Non| C[Attente]
    C -->|30s| A
    B -->|Oui| D[Enregistrement<br/>fichiers trait√©s]
    D --> E[D√©clenchement<br/>du job]
    E --> F[Traitement<br/>des fichiers]
    F --> A
```

### Perspectives d'√©volution

Pour inclure les rues urbaines dans les pr√©dictions, il faudrait:

- **Int√©grer** des sources de donn√©es compl√©mentaires (capteurs urbains, API tierces)
- **Adapter** le mod√®le de pr√©diction avec des features pertinentes pour le trafic urbain
- **Enrichir** la base de donn√©es d'entra√Ænement avec des exemples de trafic en zone urbaine

### Contributeurs

## [<img src="https://github.com/Luluscript.png" width="100px;"/><br /><sub><a href="https://github.com/Luluscript">Ludivine TULCIBIEZ</a></sub>](https://github.com/Luluscript)

## [<img src="https://github.com/naikibro.png" width="100px;"/><br /><sub><a href="https://github.com/naikibro">Vaanaiki Brotherson</a></sub>](https://github.com/naikibro)
